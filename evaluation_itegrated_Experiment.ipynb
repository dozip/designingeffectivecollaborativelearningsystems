{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaulation of Integrated Experiment\n",
    "We look at the Performance (MAE/MSE) as we did for the integrated evaluation!\n",
    "We look at the difference in different BWE Metriks and Inventory KPIs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of Performance difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "import matplotlib.ticker as ticker\n",
    "# Set the formatter for the y-axis to include commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# berechnung metriken\n",
    "def inventory_variance_ratio(df:pd.DataFrame, df_market=None, test_time=None):\n",
    "    t_minus = 200\n",
    "\n",
    "    time_interval_start = test_time\n",
    "    time_intervall_stop = test_time - t_minus \n",
    "\n",
    "    ids = df['id'].unique()\n",
    "    metric = []\n",
    "    \n",
    "    demand_data = []\n",
    "    inv_data = []\n",
    "    for id in ids:\n",
    "        inv_data.append((df[df['id']==id]['inv'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "        demand_data.append((df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "    \n",
    "    demand_data_aggregated = np.sum(demand_data, axis=0)\n",
    "    inv_data_aggregated = np.sum(inv_data, axis=0)\n",
    "\n",
    "    inv_mean = np.mean(inv_data_aggregated)\n",
    "    inv_var = np.var(inv_data_aggregated)\n",
    "    demand_mean = np.mean(demand_data_aggregated)\n",
    "    demand_var = np.var(demand_data_aggregated)\n",
    "\n",
    "    metric_aggregated = ((inv_var)/(inv_mean))/((demand_var)/(demand_mean))\n",
    "\n",
    "    # demand_data = np.array(df_market['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "    demand_data = []\n",
    "    inv_data = []\n",
    "    for id in ids:\n",
    "\n",
    "        inv_data = np.array(df[df['id']==id]['inv'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "        demand_data = np.array(df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "\n",
    "        inv_mean = np.mean(inv_data)\n",
    "        inv_var = np.var(inv_data)\n",
    "        demand_mean = np.mean(demand_data)\n",
    "        demand_var = np.var(demand_data)\n",
    "        \n",
    "        metric_id = ((inv_var)/(inv_mean))/((demand_var)/(demand_mean))\n",
    "\n",
    "        metric.append(metric_id)\n",
    "    \n",
    "    metric_sum = np.sum(metric)\n",
    "\n",
    "    return metric, metric_sum, metric_aggregated\n",
    "\n",
    "def order_variance_ratio(df:pd.DataFrame, df_market=None, test_time=None):\n",
    "    t_minus = 200\n",
    "\n",
    "    time_interval_start = test_time\n",
    "    time_intervall_stop = test_time - t_minus \n",
    "\n",
    "    ids = df['id'].unique()\n",
    "    metric = []\n",
    "\n",
    "    \n",
    "    demand_data = []\n",
    "    order_data = []\n",
    "    for id in ids:\n",
    "        order_data.append((df[df['id']==id]['order'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "        demand_data.append((df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "    \n",
    "    demand_data_aggregated = np.sum(demand_data, axis=0)\n",
    "    order_data_aggregated = np.sum(order_data, axis=0)\n",
    "\n",
    "    order_mean = np.mean(order_data_aggregated)\n",
    "    order_var = np.var(order_data_aggregated)\n",
    "    demand_mean = np.mean(demand_data_aggregated)\n",
    "    demand_var = np.var(demand_data_aggregated)\n",
    "\n",
    "    metric_aggregated = ((order_var)/(order_mean))/((demand_var)/(demand_mean))\n",
    "\n",
    "    # demand_data = np.array(df_market['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "    \n",
    "    demand_data = []\n",
    "    order_data = []\n",
    "    for id in ids:\n",
    "\n",
    "        order_data = np.array(df[df['id']==id]['order'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "        demand_data = np.array(df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "\n",
    "        order_mean = np.mean(order_data)\n",
    "        order_var = np.var(order_data)\n",
    "        demand_mean = np.mean(demand_data)\n",
    "        demand_var = np.var(demand_data)\n",
    "        \n",
    "        metric_id = ((order_var)/(order_mean))/((demand_var)/(demand_mean))\n",
    "\n",
    "        metric.append(metric_id)\n",
    "    \n",
    "    metric_sum = np.sum(metric)\n",
    "\n",
    "    return metric, metric_sum, metric_aggregated\n",
    "\n",
    "def bullwhip_ratio_order(df:pd.DataFrame, df_market=None, test_time=None):\n",
    "    t_minus = 200\n",
    "\n",
    "    time_interval_start = test_time\n",
    "    time_intervall_stop = test_time - t_minus \n",
    "\n",
    "    ids = df['id'].unique()\n",
    "    metric = []\n",
    "\n",
    "    \n",
    "    demand_data = []\n",
    "    order_data = []\n",
    "    for id in ids:\n",
    "        order_data.append((df[df['id']==id]['order'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "        demand_data.append((df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "    \n",
    "    demand_data_aggregated = np.sum(demand_data, axis=0)\n",
    "    order_data_aggregated = np.sum(order_data, axis=0)\n",
    "\n",
    "    order_var = np.var(order_data_aggregated)\n",
    "    demand_var = np.var(demand_data_aggregated)\n",
    "\n",
    "    metric_aggregated = (order_var)/(demand_var)\n",
    "\n",
    "    # demand_data = np.array(df_market['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "    \n",
    "    demand_data = []\n",
    "    order_data = []\n",
    "    for id in ids:\n",
    "\n",
    "        order_data = np.array(df[df['id']==id]['order'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "        demand_data = np.array(df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "\n",
    "    \n",
    "        order_var = np.var(order_data)\n",
    "        demand_var = np.var(demand_data)\n",
    "        \n",
    "        metric_id = (order_var)/(demand_var)\n",
    "\n",
    "        metric.append(metric_id)\n",
    "    \n",
    "    metric_sum = np.sum(metric)\n",
    "\n",
    "    return metric, metric_sum, metric_aggregated\n",
    "\n",
    "def bullwhip_ratio_inv(df:pd.DataFrame, df_market=None, test_time=None):\n",
    "    t_minus = 200\n",
    "\n",
    "    time_interval_start = test_time\n",
    "    time_intervall_stop = test_time - t_minus \n",
    "\n",
    "    ids = df['id'].unique()\n",
    "    metric = []\n",
    "\n",
    "    \n",
    "    demand_data = []\n",
    "    inv_data = []\n",
    "    for id in ids:\n",
    "        inv_data.append((df[df['id']==id]['inv'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "        demand_data.append((df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "    \n",
    "    demand_data_aggregated = np.sum(demand_data, axis=0)\n",
    "    inv_data_aggregated = np.sum(inv_data, axis=0)\n",
    "\n",
    "    inv_var = np.var(inv_data_aggregated)\n",
    "    demand_var = np.var(demand_data_aggregated)\n",
    "\n",
    "    metric_aggregated = (inv_var)/(demand_var)\n",
    "\n",
    "    # demand_data = np.array(df_market['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "    \n",
    "    demand_data = []\n",
    "    inv_data = []\n",
    "    for id in ids:\n",
    "\n",
    "        inv_data = np.array(df[df['id']==id]['inv'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "        demand_data = np.array(df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "\n",
    "    \n",
    "        inv_var = np.var(inv_data)\n",
    "        demand_var = np.var(demand_data)\n",
    "        \n",
    "        metric_id = (inv_var)/(demand_var)\n",
    "\n",
    "        metric.append(metric_id)\n",
    "    \n",
    "    metric_sum = np.sum(metric)\n",
    "\n",
    "    return metric, metric_sum, metric_aggregated\n",
    "\n",
    "def MAE(df:pd.DataFrame, test_time, test_intervall = 50):\n",
    "    t_minus = test_intervall\n",
    "\n",
    "    time_interval_start = test_time\n",
    "    time_intervall_stop = test_time - t_minus \n",
    "    ids = df['id'].unique()\n",
    "    metric = []\n",
    "\n",
    "    demand_data = []\n",
    "    forecast_data = []\n",
    "    for id in ids:\n",
    "        forecast_data.append((df[df['id']==id]['forecast'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "        demand_data.append((df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "    \n",
    "    demand_data_aggregated = np.sum(demand_data, axis=0)\n",
    "    forecast_data_aggregated = np.sum(forecast_data, axis=0)\n",
    "\n",
    "    # forecast at t-1 to demand_data at t\n",
    "    forecast_data_aggregated = forecast_data_aggregated[:-1]\n",
    "    demand_data_aggregated = demand_data_aggregated[1:]\n",
    "    metric_aggregated = mean_absolute_error(y_true=demand_data, y_pred=forecast_data)\n",
    "\n",
    "    demand_data = []\n",
    "    forecast_data = []\n",
    "    for id in ids:\n",
    "\n",
    "        forecast_data = np.array(df[df['id']==id]['forecast'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "        demand_data = np.array(df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "\n",
    "        # forecast at t-1 to demand_data at t\n",
    "        forecast_data = forecast_data[:-1]\n",
    "        demand_data = demand_data[1:]\n",
    "        \n",
    "        metric_id = mean_absolute_error(y_true=demand_data, y_pred=forecast_data)\n",
    "        metric.append(metric_id)\n",
    "    \n",
    "    metric_sum = np.sum(metric)\n",
    "\n",
    "    return metric, metric_sum, metric_aggregated\n",
    "\n",
    "def MSE(df:pd.DataFrame, test_time, test_intervall = 50):\n",
    "    t_minus = test_intervall\n",
    "\n",
    "    time_interval_start = test_time\n",
    "    time_intervall_stop = test_time - t_minus \n",
    "\n",
    "    ids = df['id'].unique()\n",
    "    metric = []\n",
    "\n",
    "    demand_data = []\n",
    "    forecast_data = []\n",
    "    for id in ids:\n",
    "        forecast_data.append((df[df['id']==id]['forecast'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "        demand_data.append((df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "    \n",
    "    demand_data_aggregated = np.sum(demand_data, axis=0)\n",
    "    forecast_data_aggregated = np.sum(forecast_data, axis=0)\n",
    "\n",
    "    # forecast at t-1 to demand_data at t\n",
    "    forecast_data_aggregated = forecast_data_aggregated[:-1]\n",
    "    demand_data_aggregated = demand_data_aggregated[1:]\n",
    "\n",
    "    metric_aggregated = mean_squared_error(y_true=demand_data, y_pred=forecast_data)\n",
    "\n",
    "    demand_data = []\n",
    "    forecast_data = []\n",
    "    for id in ids:\n",
    "\n",
    "        forecast_data = np.array(df[df['id']==id]['forecast'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "        demand_data = np.array(df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "\n",
    "        # forecast at t-1 to demand_data at t\n",
    "        forecast_data = forecast_data[:-1]\n",
    "        demand_data = demand_data[1:]\n",
    "        \n",
    "        metric_id = mean_squared_error(y_true=demand_data, y_pred=forecast_data)\n",
    "        metric.append(metric_id)\n",
    "    \n",
    "    metric_sum = np.sum(metric)\n",
    "\n",
    "    return metric, metric_sum, metric_aggregated\n",
    "\n",
    "def R2(df:pd.DataFrame, test_time, test_intervall = 50):\n",
    "    t_minus = test_intervall\n",
    "\n",
    "    time_interval_start = test_time\n",
    "    time_intervall_stop = test_time - t_minus \n",
    "\n",
    "    ids = df['id'].unique()\n",
    "    metric = []\n",
    "\n",
    "    demand_data = []\n",
    "    forecast_data = []\n",
    "    for id in ids:\n",
    "        forecast_data.append((df[df['id']==id]['forecast'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "        demand_data.append((df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop]))\n",
    "    \n",
    "    demand_data_aggregated = np.sum(demand_data, axis=0)\n",
    "    forecast_data_aggregated = np.sum(forecast_data, axis=0)\n",
    "\n",
    "    # forecast at t-1 to demand_data at t\n",
    "    forecast_data_aggregated = forecast_data_aggregated[:-1]\n",
    "    demand_data_aggregated = demand_data_aggregated[1:]\n",
    "\n",
    "    metric_aggregated = r2_score(y_true=demand_data, y_pred=forecast_data)\n",
    "\n",
    "    demand_data = []\n",
    "    forecast_data = []\n",
    "    for id in ids:\n",
    "\n",
    "        forecast_data = np.array(df[df['id']==id]['forecast'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "        demand_data = np.array(df[df['id']==id]['demand'].iloc[-time_interval_start:-time_intervall_stop])\n",
    "\n",
    "        # forecast at t-1 to demand_data at t\n",
    "        forecast_data = forecast_data[:-1]\n",
    "        demand_data = demand_data[1:]\n",
    "        \n",
    "        metric_id = r2_score(y_true=demand_data, y_pred=forecast_data)\n",
    "        metric.append(metric_id)\n",
    "    \n",
    "    metric_sum = np.sum(metric)\n",
    "\n",
    "    return metric, metric_sum, metric_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_subdirectories(base_path, runs=10, ids = 2, metric = \"IVR\", test_intervall = 50):\n",
    "    base_path = Path(base_path)\n",
    "    colums = ['path', 'sim_time', 'test_time', 'training', 'epochs', 'sequence_length', 'trend', 'freq', 'mag', 'noise', 'share', 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1']\n",
    "    colums_agent = ['path', 'sim_time', 'test_time', 'training', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share', 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1']\n",
    "    for run in range(runs):\n",
    "        colum_name = \"run_\"+str(run)\n",
    "        colums.append(colum_name)\n",
    "\n",
    "        for id in range(ids):\n",
    "            colum_name_agent = f\"run_{run}_agent_{id}\"\n",
    "            colums_agent.append(colum_name_agent)\n",
    "\n",
    "    colums.append('metric_mean')\n",
    "    # colums.append('metric_std')\n",
    "    final_list = []\n",
    "    final_agent_list = []\n",
    "    final_list_aggregated = []\n",
    "    for path in base_path.glob('*'):\n",
    "        if path.is_dir(): \n",
    "            if \"simulation\" in path.stem:\n",
    "                metric_list = []\n",
    "                metric_agent_list = []\n",
    "                metric_list_aggregated = []\n",
    "                # Open the file and load its contents\n",
    "                subpath = Path(path,\"config.json\")\n",
    "                with open(subpath, 'r') as file:\n",
    "                    config= json.load(file)\n",
    "                sim_time = config['sim']['convergence_time'] + config['sim']['simulation_time']\n",
    "                test_time = config['sim']['testing_time']\n",
    "                training_typ = config['sim']['training_type']\n",
    "                if training_typ is None:\n",
    "                    training_typ = 'None'\n",
    "                epochs = config['sim']['epochs']\n",
    "                sequence_length = config['sim']['sequence_length']\n",
    "                trend = config['market']['trend_magnitude']\n",
    "                freq = config['market']['seasonality_magnitude']\n",
    "                mag = config['market']['seasonality_frequncy']\n",
    "                noise = config['market']['random_walk']['mean']\n",
    "                share = config['market']['demand_split'][0]\n",
    "                sc_levels = config['supply_chain'][\"agents_per_level\"]\n",
    "                leadtime_level = []\n",
    "                R_level = []\n",
    "\n",
    "                for i, level in enumerate(sc_levels):\n",
    "                    key = \"sc_level_\"+str(i)\n",
    "                    lead_time_i = config['supply_chain'][\"sc_levels\"][key][\"lead_time\"]\n",
    "                    leadtime_level.extend([lead_time_i])\n",
    "  \n",
    "                    R_level_i = config['supply_chain'][\"sc_levels\"][key][\"R\"][0]\n",
    "                    R_level.extend([R_level_i])\n",
    "                for run in range(runs):\n",
    "                    run_path = \"run_\"+str(run)\n",
    "                    file_path= Path(path,run_path,\"Data\", \"agent_sc_level_1.csv\")\n",
    "                    market_path= Path(path,run_path,\"Data\", \"market.csv\")\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    df_market = pd.read_csv(market_path)\n",
    "                    if metric == \"IVR\":\n",
    "                        metric_agents, metric_sum, metric_aggregated = inventory_variance_ratio(df, df_market, test_time)\n",
    "                    elif metric == \"OVR\":\n",
    "                        metric_agents, metric_sum, metric_aggregated = order_variance_ratio(df, df_market, test_time)\n",
    "                    elif metric == \"MAE\":\n",
    "                        metric_agents, metric_sum, metric_aggregated = MAE(df, test_time, test_intervall)\n",
    "                    elif metric == \"MSE\":\n",
    "                        metric_agents, metric_sum, metric_aggregated = MSE(df, test_time, test_intervall)\n",
    "                    elif metric == \"R2\":\n",
    "                        metric_agents, metric_sum, metric_aggregated = R2(df, test_time, test_intervall)\n",
    "                    elif metric == \"BWR_order\":\n",
    "                        metric_agents, metric_sum, metric_aggregated = bullwhip_ratio_order(df, df_market, test_time)\n",
    "                    elif metric == \"BWR_inventory\":\n",
    "                        metric_agents, metric_sum, metric_aggregated = bullwhip_ratio_inv(df, df_market, test_time)\n",
    "                    else:\n",
    "                        print(\"Fallback to Default - Metric: IVR\")\n",
    "                        metric_agents, metric_sum = inventory_variance_ratio(df, df_market, test_time)\n",
    "                    metric_list.append(metric_sum)\n",
    "                    metric_agent_list.extend(metric_agents)\n",
    "                    metric_list_aggregated.append(metric_aggregated)\n",
    "\n",
    "                metric_mean = np.mean(metric_list)\n",
    "                # metric_std = np.std(metric_list)\n",
    "                data = [path.name, sim_time, test_time, training_typ, epochs, sequence_length, trend, freq, mag, noise, share]\n",
    "                data.extend(leadtime_level)\n",
    "                data.extend(R_level)\n",
    "                data.extend(metric_list)\n",
    "                data.append(metric_mean)\n",
    "                # data.append(metric_std)\n",
    "                final_list.append(data)\n",
    "\n",
    "                data_agent = [path.name, sim_time, test_time, training_typ, epochs, sequence_length,  trend, freq, mag, noise, share]\n",
    "                data_agent.extend(leadtime_level)\n",
    "                data_agent.extend(R_level)\n",
    "                data_agent.extend(metric_agent_list)\n",
    "                final_agent_list.append(data_agent)\n",
    "\n",
    "                metric_mean_aggregated = np.mean(metric_list_aggregated)\n",
    "                data_aggregated = [path.name, sim_time, test_time, training_typ, epochs, sequence_length, trend, freq, mag, noise, share]\n",
    "                data_aggregated.extend(leadtime_level)\n",
    "                data_aggregated.extend(R_level)\n",
    "                data_aggregated.extend(metric_list_aggregated)\n",
    "                data_aggregated.append(metric_mean_aggregated)\n",
    "                final_list_aggregated.append(data_aggregated)\n",
    "\n",
    "    df = pd.DataFrame(final_list, columns=colums)\n",
    "    df_agent = pd.DataFrame(final_agent_list, columns=colums_agent)\n",
    "    df_aggregated = pd.DataFrame(final_list_aggregated, columns=colums)\n",
    "         \n",
    "    return df, df_agent, df_aggregated, config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0) Functions to create evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(df:pd.DataFrame, file_path: str, sheet_name:str) -> None:\n",
    "\n",
    "     with pd.ExcelWriter(file_path, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "def compare_agent(df:pd.DataFrame, RUNS:int, IDS:int) -> pd.DataFrame:\n",
    "\n",
    "        freq_unique = np.sort(df['freq'].unique())\n",
    "        noise_unique = np.sort(df['noise'].unique())\n",
    "        share_unique = np.sort(df['share'].unique())\n",
    "        columns = []\n",
    "        colums_agent = []\n",
    "        for run in range(RUNS):\n",
    "                colum_name = \"run_\"+str(run)\n",
    "                columns.append(colum_name)\n",
    "                for id in range(IDS):\n",
    "                        colum_name_agent = f\"run_{run}_agent_{id}\"\n",
    "                        colums_agent.append(colum_name_agent)\n",
    "\n",
    "        df_final = pd.DataFrame()\n",
    "        for share in share_unique:\n",
    "                df_share = df[df['share']==share]\n",
    "                for freq in freq_unique:\n",
    "                        df_freq = df_share[df_share['freq']==freq]\n",
    "                        for noise in noise_unique:\n",
    "                                df_ = df_freq[df_freq['noise']==noise]\n",
    "                                sim_time = df_['sim_time'].unique()[0]\n",
    "                                test_time = df_['test_time'].unique()[0]\n",
    "                                epochs = df_['epochs'].unique()[0]\n",
    "                                sequence_length = df_['sequence_length'].unique()[0]\n",
    "                                trend = df_['trend'].unique()[0]\n",
    "                                freq = df_['freq'].unique()[0]\n",
    "                                mag = df_['mag'].unique()[0]\n",
    "                                noise = df_['noise'].unique()[0]\n",
    "                                share = df_['share'].unique()[0]\n",
    "                                leadtime_l0 = df_['lead_time_l0'].unique()\n",
    "                                leadtime_l1 = df_['lead_time_l1'].unique()\n",
    "                                R_l0 = df_['R_l0'].unique()\n",
    "                                R_l1 = df_['R_l1'].unique()\n",
    "                                base_sn = \"None\"\n",
    "                                base_ln = \"None\"\n",
    "                                base_sl = \"local\"\n",
    "                                compared_split = \"split_multichannel\"\n",
    "                                compared_local = \"local_multichannel\"\n",
    "\n",
    "                                split_none = np.round((df_[df_['training']=='split_multichannel'][colums_agent].iloc[0]-df_[df_['training']=='None'][colums_agent].iloc[0])/(df_[df_['training']=='None'][colums_agent].iloc[0]),2)*100\n",
    "                                local_none = np.round((df_[df_['training']=='local_multichannel'][colums_agent].iloc[0]-df_[df_['training']=='None'][colums_agent].iloc[0])/(df_[df_['training']=='None'][colums_agent].iloc[0]),2)*100\n",
    "                                split_local = np.round((df_[df_['training']=='split_multichannel'][colums_agent].iloc[0]-df_[df_['training']=='local_multichannel'][colums_agent].iloc[0])/(df_[df_['training']=='local_multichannel'][colums_agent].iloc[0]),2)*100\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs, sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_sn, compared_split]).transpose()\n",
    "                                df_new.columns =  [\"sim_time\", \"test_time\", \"epochs\",\"sequence_length\", \"trend\", \"freq\", \"mag\", \"noise\", \"share\", 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "                                df_split_none = pd.DataFrame(data=split_none).transpose()\n",
    "                                df_split_none['mean'] = np.mean(split_none)\n",
    "                                df_split_none = pd.concat([df_new, df_split_none], axis=1)\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_ln, compared_local]).transpose()\n",
    "                                df_new.columns =  [\"sim_time\", \"test_time\", \"epochs\",\"sequence_length\", \"trend\", \"freq\", \"mag\", \"noise\", \"share\", 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "                                df_local_none = pd.DataFrame(data=local_none).transpose()\n",
    "                                df_local_none['mean'] = np.mean(local_none)\n",
    "                                df_local_none = pd.concat([df_new, df_local_none], axis=1)\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1,  base_sl, compared_split]).transpose()\n",
    "                                df_new.columns = [\"sim_time\", \"test_time\", \"epochs\", \"sequence_length\",\"trend\", \"freq\", \"mag\", \"noise\", \"share\", 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "                                df_split_local = pd.DataFrame(data=split_local).transpose()\n",
    "                                df_split_local['mean'] = np.mean(split_local)\n",
    "                                df_split_local = pd.concat([df_new, df_split_local], axis=1)\n",
    "                                \n",
    "                                df_final = pd.concat([df_final, df_split_none, df_local_none, df_split_local], axis=0)\n",
    "\n",
    "        return df_final\n",
    "\n",
    "def pareto(df:pd.DataFrame, RUNS:int, IDS:int)->pd.DataFrame:\n",
    "\n",
    "        data = []\n",
    "        colums_constant = [\"sim_time\", \"test_time\", \"epochs\",\"sequence_length\", \"trend\", \"freq\", \"mag\", \"noise\", \"share\", 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "        df = df.reset_index(drop=True)\n",
    "        df_constant = df[colums_constant]\n",
    "        column = []\n",
    "        for run in range(RUNS):\n",
    "                colum_name = \"run_\"+str(run)\n",
    "                column.append(colum_name)\n",
    "                c_list = []\n",
    "        \n",
    "                for id in range(IDS):\n",
    "                        colum_name_agent = f\"run_{run}_agent_{id}\"\n",
    "                        c_list.append(colum_name_agent)\n",
    "                rows_less_than_zero = df[c_list].lt(0).all(axis=1).tolist()\n",
    "                data.append(rows_less_than_zero)\n",
    "\n",
    "        df_final_agent = pd.DataFrame(data=data).transpose()\n",
    "        df_final_agent.columns = column\n",
    "        df_new = df_constant.join(df_final_agent)\n",
    "\n",
    "        agent_aggr = []\n",
    "        for id in range(IDS):\n",
    "                columns = []\n",
    "        \n",
    "                for run in range(RUNS):\n",
    "                        column = f\"run_{run}_agent_{id}\"\n",
    "                        columns.append(column)\n",
    "        df_constant = df[columns]\n",
    "        agent_aggr.append(df_constant.mean(axis=1).values)\n",
    "\n",
    "        d =pd.DataFrame(agent_aggr).transpose()\n",
    "        d = d.lt(0).all(axis=1).tolist()\n",
    "\n",
    "        df_new['aggregated']=d\n",
    "\n",
    "        return df_new\n",
    "\n",
    "def compare_runs(df:pd.DataFrame, RUNS:int, IDS:int):\n",
    "        freq_unique = np.sort(df['freq'].unique())\n",
    "        noise_unique = np.sort(df['noise'].unique())\n",
    "        share_unique = np.sort(df['share'].unique())\n",
    "        columns = []\n",
    "        df_final = pd.DataFrame()\n",
    "        for i in range(RUNS):\n",
    "                name = \"run_\"+str(i)\n",
    "                columns.append(name)\n",
    "\n",
    "        for share in share_unique:\n",
    "                df_share = df[df['share']==share]\n",
    "                for freq in freq_unique:\n",
    "                        df_freq = df_share[df_share['freq']==freq]\n",
    "                        for noise in noise_unique:\n",
    "                                df_ = df_freq[df_freq['noise']==noise]\n",
    "                                sim_time = df_['sim_time'].unique()[0]\n",
    "                                test_time = df_['test_time'].unique()[0]\n",
    "                                epochs = df_['epochs'].unique()[0]\n",
    "                                sequence_length = df_['sequence_length'].unique()[0]\n",
    "                                trend = df_['trend'].unique()[0]\n",
    "                                freq = df_['freq'].unique()[0]\n",
    "                                mag = df_['mag'].unique()[0]\n",
    "                                noise = df_['noise'].unique()[0]\n",
    "                                share = df_['share'].unique()[0]\n",
    "                                leadtime_l0 = df_['lead_time_l0'].unique()\n",
    "                                leadtime_l1 = df_['lead_time_l1'].unique()\n",
    "                                R_l0 = df_['R_l0'].unique()\n",
    "                                R_l1 = df_['R_l1'].unique()\n",
    "                                base_sn = \"None\"\n",
    "                                base_ln = \"None\"\n",
    "                                base_sl = \"local\"\n",
    "                                compared_split = \"split_multichannel\"\n",
    "                                compared_local = \"local_multichannel\"\n",
    "                                \n",
    "                                split_none = np.round((df_[df_['training']=='split_multichannel'][columns].iloc[0]-df_[df_['training']=='None'][columns].iloc[0])/(df_[df_['training']=='None'][columns].iloc[0]),2)*100\n",
    "                                local_none = np.round((df_[df_['training']=='local_multichannel'][columns].iloc[0]-df_[df_['training']=='None'][columns].iloc[0])/(df_[df_['training']=='None'][columns].iloc[0]),2)*100\n",
    "                                split_local = np.round((df_[df_['training']=='split_multichannel'][columns].iloc[0]-df_[df_['training']=='local_multichannel'][columns].iloc[0])/(df_[df_['training']=='local_multichannel'][columns].iloc[0]),2)*100\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_sn, compared_split]).transpose()\n",
    "                                df_new.columns =  ['sim_time', 'test_time', 'epochs', 'sequence_length','trend', 'freq', 'mag', 'noise', 'share','lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1',\"base\", \"compared with\"]\n",
    "                                df_split_none = pd.DataFrame(data=split_none).transpose()\n",
    "                                df_split_none['mean'] = np.mean(split_none)\n",
    "                                df_split_none = pd.concat([df_new, df_split_none], axis=1)\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_ln, compared_local]).transpose()\n",
    "                                df_new.columns =  ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share','lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "                                df_local_none = pd.DataFrame(data=local_none).transpose()\n",
    "                                df_local_none['mean'] = np.mean(local_none)\n",
    "                                df_local_none = pd.concat([df_new, df_local_none], axis=1)\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_sl, compared_split]).transpose()\n",
    "                                df_new.columns =  ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share','lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1',\"base\", \"compared with\"]\n",
    "                                df_split_local = pd.DataFrame(data=split_local).transpose()\n",
    "                                df_split_local['mean'] = np.mean(split_local)\n",
    "                                df_split_local = pd.concat([df_new, df_split_local], axis=1)\n",
    "                                \n",
    "                                df_final = pd.concat([df_final, df_split_none, df_local_none, df_split_local], axis=0)\n",
    "            \n",
    "\n",
    "        return df_final\n",
    "\n",
    "def compare_runs_raw_values(df:pd.DataFrame, RUNS:int, IDS:int):\n",
    "        freq_unique = np.sort(df['freq'].unique())\n",
    "        noise_unique = np.sort(df['noise'].unique())\n",
    "        share_unique = np.sort(df['share'].unique())\n",
    "        columns = []\n",
    "        df_final = pd.DataFrame()\n",
    "        for i in range(RUNS):\n",
    "                name = \"run_\"+str(i)\n",
    "                columns.append(name)\n",
    "\n",
    "        for share in share_unique:\n",
    "                df_share = df[df['share']==share]\n",
    "                for freq in freq_unique:\n",
    "                        df_freq = df_share[df_share['freq']==freq]\n",
    "                        for noise in noise_unique:\n",
    "                                df_ = df_freq[df_freq['noise']==noise]\n",
    "                                sim_time = df_['sim_time'].unique()[0]\n",
    "                                test_time = df_['test_time'].unique()[0]\n",
    "                                epochs = df_['epochs'].unique()[0]\n",
    "                                sequence_length = df_['sequence_length'].unique()[0]\n",
    "                                trend = df_['trend'].unique()[0]\n",
    "                                freq = df_['freq'].unique()[0]\n",
    "                                mag = df_['mag'].unique()[0]\n",
    "                                noise = df_['noise'].unique()[0]\n",
    "                                share = df_['share'].unique()[0]\n",
    "                                leadtime_l0 = df_['lead_time_l0'].unique()\n",
    "                                leadtime_l1 = df_['lead_time_l1'].unique()\n",
    "                                R_l0 = df_['R_l0'].unique()\n",
    "                                R_l1 = df_['R_l1'].unique()\n",
    "                                base_sn = \"None\"\n",
    "                                base_ln = \"None\"\n",
    "                                base_sl = \"local\"\n",
    "                                compared_split = \"split_multichannel\"\n",
    "                                compared_local = \"local_multichannel\"\n",
    "                                \n",
    "                                split_none = np.round((df_[df_['training']=='split_multichannel'][columns].iloc[0]-df_[df_['training']=='None'][columns].iloc[0]))\n",
    "                                local_none = np.round((df_[df_['training']=='local_multichannel'][columns].iloc[0]-df_[df_['training']=='None'][columns].iloc[0]))\n",
    "                                split_local = np.round((df_[df_['training']=='split_multichannel'][columns].iloc[0]-df_[df_['training']=='local_multichannel'][columns].iloc[0]))\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_sn, compared_split]).transpose()\n",
    "                                df_new.columns =  ['sim_time', 'test_time', 'epochs', 'sequence_length','trend', 'freq', 'mag', 'noise', 'share','lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1',\"base\", \"compared with\"]\n",
    "                                df_split_none = pd.DataFrame(data=split_none).transpose()\n",
    "                                df_split_none['mean'] = np.mean(split_none)\n",
    "                                df_split_none = pd.concat([df_new, df_split_none], axis=1)\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_ln, compared_local]).transpose()\n",
    "                                df_new.columns =  ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share','lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "                                df_local_none = pd.DataFrame(data=local_none).transpose()\n",
    "                                df_local_none['mean'] = np.mean(local_none)\n",
    "                                df_local_none = pd.concat([df_new, df_local_none], axis=1)\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_sl, compared_split]).transpose()\n",
    "                                df_new.columns =  ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share','lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1',\"base\", \"compared with\"]\n",
    "                                df_split_local = pd.DataFrame(data=split_local).transpose()\n",
    "                                df_split_local['mean'] = np.mean(split_local)\n",
    "                                df_split_local = pd.concat([df_new, df_split_local], axis=1)\n",
    "                                \n",
    "                                df_final = pd.concat([df_final, df_split_none, df_local_none, df_split_local], axis=0)\n",
    "            \n",
    "\n",
    "        return df_final\n",
    "\n",
    "def system_raw_values(df:pd.DataFrame):\n",
    "        freq_unique = np.sort(df['freq'].unique())\n",
    "        noise_unique = np.sort(df['noise'].unique())\n",
    "        share_unique = np.sort(df['share'].unique())\n",
    "        df_final = pd.DataFrame()\n",
    "        for share in share_unique:\n",
    "                df_share = df[df['share']==share]\n",
    "                for freq in freq_unique:\n",
    "                        df_freq = df_share[df_share['freq']==freq]\n",
    "                        for noise in noise_unique:\n",
    "                                df_ = df_freq[df_freq['noise']==noise]\n",
    "                                sim_time = df_['sim_time'].unique()[0]\n",
    "                                test_time = df_['test_time'].unique()[0]\n",
    "                                epochs = df_['epochs'].unique()[0]\n",
    "                                sequence_length = df_['sequence_length'].unique()[0]\n",
    "                                trend = df_['trend'].unique()[0]\n",
    "                                freq = df_['freq'].unique()[0]\n",
    "                                mag = df_['mag'].unique()[0]\n",
    "                                noise = df_['noise'].unique()[0]\n",
    "                                share = df_['share'].unique()[0]\n",
    "                                leadtime_l0 = df_['lead_time_l0'].unique()\n",
    "                                leadtime_l1 = df_['lead_time_l1'].unique()\n",
    "                                R_l0 = df_['R_l0'].unique()\n",
    "                                R_l1 = df_['R_l1'].unique()\n",
    "                                metric_MA = df_[df_['training']=='None']['metric_mean'].values[0]\n",
    "                                metric_local = df_[df_['training']=='local_multichannel']['metric_mean'].values[0]\n",
    "                                metric_split = df_[df_['training']=='split_multichannel']['metric_mean'].values[0]\n",
    "\n",
    "                                df_new = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1]).transpose()\n",
    "                                df_new.columns =  ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share', 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1']\n",
    "                                df_new['metric_MA'] = metric_MA\n",
    "                                df_new['metric_local'] = metric_local\n",
    "                                df_new['metric_split'] = metric_split\n",
    "                                df_final = pd.concat([df_final, df_new], axis=0)\n",
    "\n",
    "        return df_final\n",
    "\n",
    "def pairedTtest(df:pd.DataFrame, RUNS:int)-> pd.DataFrame:\n",
    "        freq_unique = np.sort(df['freq'].unique())\n",
    "        noise_unique = np.sort(df['noise'].unique())\n",
    "        share_unique = np.sort(df['share'].unique())\n",
    "        columns = []\n",
    "        df_final = pd.DataFrame()\n",
    "        for i in range(RUNS):\n",
    "                name = \"run_\"+str(i)\n",
    "                columns.append(name)\n",
    "        for share in share_unique:\n",
    "                df_share = df[df['share']==share]\n",
    "                for freq in freq_unique:\n",
    "                        df_freq = df_share[df_share['freq']==freq]\n",
    "                        for noise in noise_unique:\n",
    "                                df_ = df_freq[df_freq['noise']==noise]\n",
    "                                sim_time = df_['sim_time'].unique()[0]\n",
    "                                test_time = df_['test_time'].unique()[0]\n",
    "                                epochs = df_['epochs'].unique()[0]\n",
    "                                sequence_length = df_['sequence_length'].unique()[0]\n",
    "                                trend = df_['trend'].unique()[0]\n",
    "                                freq = df_['freq'].unique()[0]\n",
    "                                mag = df_['mag'].unique()[0]\n",
    "                                noise = df_['noise'].unique()[0]\n",
    "                                share = df_['share'].unique()[0]\n",
    "                                leadtime_l0 = df_['lead_time_l0'].unique()\n",
    "                                leadtime_l1 = df_['lead_time_l1'].unique()\n",
    "                                R_l0 = df_['R_l0'].unique()\n",
    "                                R_l1 = df_['R_l1'].unique()\n",
    "\n",
    "                                base_sn = \"None\"\n",
    "                                base_ln = \"None\"\n",
    "                                base_sl = \"local\"\n",
    "                                compared_split = \"split_multichannel\"\n",
    "                                compared_local = \"local_multichannel\"\n",
    "                                \n",
    "                                split = df_[df_['training']=='split_multichannel'][columns].iloc[0]\n",
    "                                local = df_[df_['training']=='local_multichannel'][columns].iloc[0]\n",
    "                                none = df_[df_['training']=='None'][columns].iloc[0]\n",
    "\n",
    "                                # test split - none\n",
    "                                T, p_sn = stats.ttest_rel(split, none, alternative='less')\n",
    "                                # test local - none\n",
    "                                T, p_ln = stats.ttest_rel(local, none, alternative='less')\n",
    "                                # test split - local\n",
    "                                T, p_sl = stats.ttest_rel(split, local, alternative='less')\n",
    "                                \n",
    "                                df_split_none= pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_sn, compared_split]).transpose()\n",
    "                                df_split_none.columns =  ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share', 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "                                df_split_none[\"p_value\"] = p_sn\n",
    "                                df_split_none[\"5%\"] = p_sn<0.05\n",
    "                                df_split_none[\"1%\"] = p_sn<0.01\n",
    "                                df_split_none[\"0.1%\"] = p_sn<0.001\n",
    "\n",
    "                                if p_sn<0.001:\n",
    "                                        df_split_none[\"sig_level\"] = \"***\"\n",
    "                                elif p_sn<0.01:\n",
    "                                        df_split_none[\"sig_level\"] = \"**\"\n",
    "                                elif p_sn<0.05:\n",
    "                                        df_split_none[\"sig_level\"] = \"*\"\n",
    "                                else:\n",
    "                                        df_split_none[\"sig_level\"] = \"n.s\"\n",
    "\n",
    "\n",
    "                                df_local_none = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1,base_ln, compared_local]).transpose()\n",
    "                                df_local_none.columns =  ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share', 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1',  \"base\", \"compared with\"]\n",
    "                                df_local_none[\"p_value\"] = p_ln\n",
    "                                df_local_none[\"5%\"] = p_ln<0.05\n",
    "                                df_local_none[\"1%\"] = p_ln<0.01\n",
    "                                df_local_none[\"0.1%\"] = p_ln<0.001\n",
    "\n",
    "                                if p_ln<0.001:\n",
    "                                        df_local_none[\"sig_level\"] = \"***\"\n",
    "                                elif p_ln<0.01:\n",
    "                                        df_local_none[\"sig_level\"] = \"**\"\n",
    "                                elif p_ln<0.05:\n",
    "                                        df_local_none[\"sig_level\"] = \"*\"\n",
    "                                else:\n",
    "                                        df_local_none[\"sig_level\"] = \"n.s\"\n",
    "\n",
    "                                df_split_local = pd.DataFrame(data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share,leadtime_l0, leadtime_l1, R_l0, R_l1, base_sl, compared_split]).transpose()\n",
    "                                df_split_local.columns =  ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share', 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "                                df_split_local [\"p_value\"] = p_sl\n",
    "                                df_split_local[\"5%\"] = p_sl<0.05\n",
    "                                df_split_local[\"1%\"] = p_sl<0.01\n",
    "                                df_split_local[\"0.1%\"] = p_sl<0.001\n",
    "\n",
    "                                if p_sl<0.001:\n",
    "                                        df_split_local[\"sig_level\"] = \"***\"\n",
    "                                elif p_sl<0.01:\n",
    "                                        df_split_local[\"sig_level\"] = \"**\"\n",
    "                                elif p_sl<0.05:\n",
    "                                        df_split_local[\"sig_level\"] = \"*\"\n",
    "                                else:\n",
    "                                        df_split_local[\"sig_level\"] = \"n.s\"\n",
    "                                \n",
    "                                df_final = pd.concat([df_final, df_split_none, df_local_none, df_split_local], axis=0)\n",
    "\n",
    "        return df_final\n",
    "\n",
    "def pairedTtestAgent(df:pd.DataFrame, RUNS:int, num_agents:int = 3)-> pd.DataFrame: # ToDo: Funktion anpassen\n",
    "        freq_unique = np.sort(df['freq'].unique())\n",
    "        noise_unique = np.sort(df['noise'].unique())\n",
    "        share_unique = np.sort(df['share'].unique())\n",
    "        df_final = pd.DataFrame()\n",
    "        columns = {}\n",
    "        for agent_id in range(num_agents):\n",
    "                column = []\n",
    "                for i in range(RUNS):\n",
    "                        name = \"run_\"+str(i)+\"_agent_\"+str(agent_id)\n",
    "                        column.append(name)\n",
    "                columns[agent_id] = column\n",
    "        for share in share_unique:\n",
    "                df_share = df[df['share']==share]\n",
    "                for freq in freq_unique:\n",
    "                        df_freq = df_share[df_share['freq']==freq]\n",
    "                        for noise in noise_unique:\n",
    "                                df_ = df_freq[df_freq['noise']==noise]\n",
    "                                sim_time = df_['sim_time'].unique()[0]\n",
    "                                test_time = df_['test_time'].unique()[0]\n",
    "                                epochs = df_['epochs'].unique()[0]\n",
    "                                sequence_length = df_['sequence_length'].unique()[0]\n",
    "                                trend = df_['trend'].unique()[0]\n",
    "                                freq = df_['freq'].unique()[0]\n",
    "                                mag = df_['mag'].unique()[0]\n",
    "                                noise = df_['noise'].unique()[0]\n",
    "                                share = df_['share'].unique()[0]\n",
    "                                leadtime_l0 = df_['lead_time_l0'].unique()\n",
    "                                leadtime_l1 = df_['lead_time_l1'].unique()\n",
    "                                R_l0 = df_['R_l0'].unique()\n",
    "                                R_l1 = df_['R_l1'].unique()\n",
    "\n",
    "                                base_sn = \"None\"\n",
    "                                base_ln = \"None\"\n",
    "                                base_sl = \"local\"\n",
    "                                compared_split = \"split_multichannel\"\n",
    "                                compared_local = \"local_multichannel\"\n",
    "\n",
    "                                p_columns = []\n",
    "                                p_sn_list = []\n",
    "                                p_ln_list = []\n",
    "                                p_sl_list = []\n",
    "                                \n",
    "                                for agent_id in range(num_agents):\n",
    "                                        p_columns.append(\"p_value_agent_\"+str(agent_id))\n",
    "                                        \n",
    "                                        \n",
    "                                        split = df_[df_['training']=='split_multichannel'][columns[agent_id]].iloc[0]\n",
    "                                        local = df_[df_['training']=='local_multichannel'][columns[agent_id]].iloc[0]\n",
    "                                        none = df_[df_['training']=='None'][columns[agent_id]].iloc[0]\n",
    "\n",
    "\n",
    "                                        # test split - none\n",
    "                                        T, p_sn = stats.ttest_rel(split, none, alternative='less')\n",
    "                                        # test local - none\n",
    "                                        T, p_ln = stats.ttest_rel(local, none, alternative='less')\n",
    "                                        # test split - local\n",
    "                                        T, p_sl = stats.ttest_rel(split, local, alternative='less')\n",
    "\n",
    "                                        p_sn_list.append(p_sn)\n",
    "                                        p_ln_list.append(p_ln)\n",
    "                                        p_sl_list.append(p_sl)\n",
    "                                \n",
    "                                colums_df = ['sim_time', 'test_time', 'epochs','sequence_length', 'trend', 'freq', 'mag', 'noise', 'share', 'lead_time_l0', 'lead_time_l1',\n",
    "              'R_l0', 'R_l1', \"base\", \"compared with\"]\n",
    "                                colums_df.extend(p_columns)\n",
    "\n",
    "                                data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_sn, compared_split]\n",
    "                                data.extend(p_sn_list)\n",
    "                                df_split_none= pd.DataFrame(data=data).transpose()\n",
    "                                df_split_none.columns =  colums_df\n",
    "                                df_split_none[\"5%\"] = all(x < 0.05 for x in p_sn_list)\n",
    "                                df_split_none[\"1%\"] = all(x < 0.01 for x in p_sn_list)\n",
    "                                df_split_none[\"0.1%\"] = all(x < 0.001 for x in p_sn_list)\n",
    "\n",
    "                                if all(x < 0.05 for x in p_sn_list):\n",
    "                                        df_split_none[\"sig_level\"] = \"***\"\n",
    "                                elif all(x < 0.05 for x in p_sn_list):\n",
    "                                        df_split_none[\"sig_level\"] = \"**\"\n",
    "                                elif all(x < 0.05 for x in p_sn_list):\n",
    "                                        df_split_none[\"sig_level\"] = \"*\"\n",
    "                                else:\n",
    "                                        df_split_none[\"sig_level\"] = \"n.s\"\n",
    "\n",
    "                                data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_ln, compared_local]\n",
    "                                data.extend(p_ln_list)\n",
    "                                df_local_none = pd.DataFrame(data=data).transpose()\n",
    "                                df_local_none.columns =  colums_df\n",
    "                                df_local_none[\"5%\"] = all(x < 0.05 for x in p_ln_list)\n",
    "                                df_local_none[\"1%\"] = all(x < 0.01 for x in p_ln_list)\n",
    "                                df_local_none[\"0.1%\"] = all(x < 0.001 for x in p_ln_list)\n",
    "\n",
    "                                if all(x < 0.05 for x in p_ln_list):\n",
    "                                        df_local_none[\"sig_level\"] = \"***\"\n",
    "                                elif all(x < 0.05 for x in p_ln_list):\n",
    "                                        df_local_none[\"sig_level\"] = \"**\"\n",
    "                                elif all(x < 0.05 for x in p_ln_list):\n",
    "                                        df_local_none[\"sig_level\"] = \"*\"\n",
    "                                else:\n",
    "                                        df_local_none[\"sig_level\"] = \"n.s\"\n",
    "\n",
    "                                data=[sim_time, test_time, epochs,sequence_length, trend, freq, mag, noise, share, leadtime_l0, leadtime_l1, R_l0, R_l1, base_sl, compared_split]\n",
    "                                data.extend(p_sl_list)\n",
    "                                df_split_local = pd.DataFrame(data=data).transpose()\n",
    "                                df_split_local.columns =  colums_df\n",
    "                                df_split_local[\"5%\"] = all(x < 0.05 for x in p_sl_list)\n",
    "                                df_split_local[\"1%\"] = all(x < 0.01 for x in p_sl_list)\n",
    "                                df_split_local[\"0.1%\"] = all(x < 0.001 for x in p_sl_list)\n",
    "\n",
    "                                if all(x < 0.05 for x in p_sl_list):\n",
    "                                        df_split_local[\"sig_level\"] = \"***\"\n",
    "                                elif all(x < 0.05 for x in p_sl_list):\n",
    "                                        df_split_local[\"sig_level\"] = \"**\"\n",
    "                                elif all(x < 0.05 for x in p_sl_list):\n",
    "                                        df_split_local[\"sig_level\"] = \"*\"\n",
    "                                else:\n",
    "                                        df_split_local[\"sig_level\"] = \"n.s\"\n",
    "                                \n",
    "                                df_final = pd.concat([df_final, df_split_none, df_local_none, df_split_local], axis=0)\n",
    "\n",
    "\n",
    "        return df_final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Evaluation: raw data and comparison of on different levels (agents, runs_ind, runs_aggr, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"Results/Rawdata/Experiment_I\", \"Results/Rawdata/Experiment_II\", \"Results/Rawdata/Experiment_III\"]\n",
    "file_paths = [\"Results/Evaluation/Evaluation_Overview_Experiment_I.xlsx\", \"Results/Evaluation/Evaluation_Overview_Experiment_II.xlsx\", \"Results/Evaluation/Evaluation_Overview_Experiment_III.xlsx\"]\n",
    "\n",
    "from openpyxl import Workbook\n",
    "\n",
    "for file_path in file_paths:\n",
    "    # Ein neues Workbook erstellen\n",
    "    wb = Workbook()\n",
    "\n",
    "    # Optional: Ein Blatt hinzufügen (Ein leeres Workbook hat standardmäßig ein Blatt)\n",
    "    ws = wb.active\n",
    "    ws.title = \"LeeresBlatt\"\n",
    "\n",
    "    # Die Excel-Datei speichern\n",
    "    wb.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: MAE\n",
      "MAE_raw_run_ind\n",
      "MAE_raw_agent\n",
      "MAE_raw_agent_aggr\n",
      "MAE_compar_agent\n",
      "MAE_pareto\n",
      "MAE_compar_run_ind\n",
      "MAE_compar_run_ind_raw\n",
      "MAE_market_level\n",
      "MAE_pTtest_run_ind\n",
      "MAE_Agent_pTtest_run_ind\n",
      "Metric: MSE\n",
      "MSE_raw_run_ind\n",
      "MSE_raw_agent\n",
      "MSE_raw_agent_aggr\n",
      "MSE_compar_agent\n",
      "MSE_pareto\n",
      "MSE_compar_run_ind\n",
      "MSE_compar_run_ind_raw\n",
      "MSE_market_level\n",
      "MSE_pTtest_run_ind\n",
      "MSE_Agent_pTtest_run_ind\n",
      "Metric: MAE\n",
      "MAE_raw_run_ind\n",
      "MAE_raw_agent\n",
      "MAE_raw_agent_aggr\n",
      "MAE_compar_agent\n",
      "MAE_pareto\n",
      "MAE_compar_run_ind\n",
      "MAE_compar_run_ind_raw\n",
      "MAE_market_level\n",
      "MAE_pTtest_run_ind\n",
      "MAE_Agent_pTtest_run_ind\n",
      "Metric: MSE\n",
      "MSE_raw_run_ind\n",
      "MSE_raw_agent\n",
      "MSE_raw_agent_aggr\n",
      "MSE_compar_agent\n",
      "MSE_pareto\n",
      "MSE_compar_run_ind\n",
      "MSE_compar_run_ind_raw\n",
      "MSE_market_level\n",
      "MSE_pTtest_run_ind\n",
      "MSE_Agent_pTtest_run_ind\n",
      "Metric: MAE\n",
      "MAE_raw_run_ind\n",
      "MAE_raw_agent\n",
      "MAE_raw_agent_aggr\n",
      "MAE_compar_agent\n",
      "MAE_pareto\n",
      "MAE_compar_run_ind\n",
      "MAE_compar_run_ind_raw\n",
      "MAE_market_level\n",
      "MAE_pTtest_run_ind\n",
      "MAE_Agent_pTtest_run_ind\n",
      "Metric: MSE\n",
      "MSE_raw_run_ind\n",
      "MSE_raw_agent\n",
      "MSE_raw_agent_aggr\n",
      "MSE_compar_agent\n",
      "MSE_pareto\n",
      "MSE_compar_run_ind\n",
      "MSE_compar_run_ind_raw\n",
      "MSE_market_level\n",
      "MSE_pTtest_run_ind\n",
      "MSE_Agent_pTtest_run_ind\n"
     ]
    }
   ],
   "source": [
    "for i, path in enumerate(paths):\n",
    "    IDS = 3\n",
    "    RUNS = 10\n",
    "    # metrics = ['IVR', 'OVR', 'BWR_order','BWR_inventory', 'MAE']\n",
    "    metrics = ['MAE', 'MSE']\n",
    "    file_path = file_paths[i]\n",
    "    for metric in metrics:\n",
    "        print(f\"Metric: {metric}\")\n",
    "        df, df_agent, df_aggregated, config = list_all_subdirectories(path, runs=10, ids=IDS, metric = metric)\n",
    "\n",
    "        # save raw values\n",
    "        print(f\"{metric}_raw_run_ind\")\n",
    "        sheet_name = f\"{metric}_raw_run_ind\"\n",
    "        write(df,file_path=file_path, sheet_name=sheet_name)\n",
    "\n",
    "        print(f\"{metric}_raw_agent\")\n",
    "        sheet_name = f\"{metric}_raw_agent\"\n",
    "        write(df_agent,file_path=file_path, sheet_name=sheet_name)\n",
    "\n",
    "        print(f\"{metric}_raw_agent_aggr\")\n",
    "        sheet_name = f\"{metric}_raw_agent_aggr\"\n",
    "        write(df_aggregated,file_path=file_path, sheet_name=sheet_name)\n",
    "\n",
    "        print(f\"{metric}_compar_agent\")\n",
    "        sheet_name = f\"{metric}_compar_agent\"\n",
    "        df_final_c_a = compare_agent(df=df_agent, RUNS=RUNS, IDS=IDS)\n",
    "        write(df_final_c_a, file_path, sheet_name)\n",
    "\n",
    "        print(f\"{metric}_pareto\")\n",
    "        sheet_name = f\"{metric}_pareto\"\n",
    "        df_final_pareto = pareto(df=df_final_c_a, RUNS=RUNS, IDS=IDS)\n",
    "        write(df_final_pareto, file_path, sheet_name)\n",
    "\n",
    "        print(f\"{metric}_compar_run_ind\")\n",
    "        sheet_name = f\"{metric}_compar_run_ind\"\n",
    "        df_final_c_run = compare_runs(df=df, RUNS=RUNS, IDS=IDS)\n",
    "        write(df_final_c_run, file_path, sheet_name)\n",
    "\n",
    "        print(f\"{metric}_compar_run_ind_raw\")\n",
    "        sheet_name = f\"{metric}_compar_run_ind_raw\"\n",
    "        df_final_c_run_raw = compare_runs_raw_values(df=df, RUNS=RUNS, IDS=IDS)\n",
    "        # write(df_final_c_run, file_path, sheet_name)\n",
    "\n",
    "        print(f\"{metric}_market_level\")\n",
    "        sheet_name = f\"{metric}_market_level\"\n",
    "        df_final_m_l = system_raw_values(df)\n",
    "        write(df_final_m_l, file_path, sheet_name)\n",
    "\n",
    "        print(f\"{metric}_pTtest_run_ind\")\n",
    "        sheet_name = f\"{metric}_pTtest_run_ind\"\n",
    "        df_final_pT = pairedTtest(df, RUNS=RUNS)\n",
    "        write(df_final_pT, file_path, sheet_name)\n",
    "\n",
    "        print(f\"{metric}_Agent_pTtest_run_ind\")\n",
    "        sheet_name = f\"{metric}_Agent_pTtest_run_ind\"\n",
    "        test_df_agent_Ttest = pairedTtestAgent(df_agent, RUNS=10)\n",
    "        write(test_df_agent_Ttest, file_path, sheet_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reporting of paired T-test - system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_scenario(freq, noise):\n",
    "    scenario = \"\"\n",
    "    if freq == 5:\n",
    "        if noise == 0:\n",
    "            scenario = f\"Scenario 1 ({freq}, {noise})\"\n",
    "        if noise == 10:\n",
    "            scenario = f\"Scenario 2 ({freq}, {noise})\"\n",
    "        if noise == 100:\n",
    "            scenario = f\"Scenario 3 ({freq}, {noise})\"\n",
    "    \n",
    "    if freq == 15:\n",
    "        if noise == 0:\n",
    "            scenario = f\"Scenario 4 ({freq}, {noise})\"\n",
    "        if noise == 10:\n",
    "            scenario = f\"Scenario 5 ({freq}, {noise})\"\n",
    "        if noise == 100:\n",
    "            scenario = f\"Scenario 6 ({freq}, {noise})\"\n",
    "\n",
    "    if freq == 30:\n",
    "        if noise == 0:\n",
    "            scenario = f\"Scenario 7 ({freq}, {noise})\"\n",
    "        if noise == 10:\n",
    "            scenario = f\"Scenario 8 ({freq}, {noise})\"\n",
    "        if noise == 100:\n",
    "            scenario = f\"Scenario 9 ({freq}, {noise})\"\n",
    "\n",
    "    return scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_table(df_system: pd.DataFrame, RUNS=10):\n",
    "    df_final = pd.DataFrame()\n",
    "    columns = []\n",
    "\n",
    "    freq_unique = np.sort(df_system['freq'].unique())\n",
    "    noise_unique = np.sort(df_system['noise'].unique())\n",
    "    share_unique = np.sort(df_system['share'].unique())\n",
    "    leadtimes_unique = np.sort(df_system['lead_time_l0'].unique())\n",
    "\n",
    "    for i in range(RUNS):\n",
    "        name = \"run_\"+str(i)\n",
    "        columns.append(name)\n",
    "\n",
    "\n",
    "    for l, leadtime in enumerate(leadtimes_unique):\n",
    "        df_L = pd.DataFrame()\n",
    "        df_leadtime = df_system[df_system['lead_time_l0']==leadtime]\n",
    "        for i, share in enumerate(share_unique):\n",
    "            df_share = df_leadtime[df_leadtime['share']==share]\n",
    "            for j, freq in enumerate(freq_unique):\n",
    "                df_freq = df_share[df_share['freq']==freq]\n",
    "                for k, noise in enumerate(noise_unique):\n",
    "                    df_ = df_freq[df_freq['noise']==noise]\n",
    "\n",
    "                    scenario = return_scenario(freq=freq, noise=noise)\n",
    "                    # leadtime_column = f\"L = {leadtime}\"\n",
    "                    leadtime_column = \"Leadtime\"\n",
    "                    base_sn = \"MAncl\"\n",
    "                    base_ln = \"MAncl\"\n",
    "                    base_sl = \"MLncl\"\n",
    "                    compared_split = \"MLcl\"\n",
    "                    compared_local = \"MLncl\"\n",
    "                    \n",
    "                    split = df_[df_['training']=='MLcl'][columns].iloc[0]\n",
    "                    local = df_[df_['training']=='MLncl'][columns].iloc[0]\n",
    "                    none = df_[df_['training']=='MAncl'][columns].iloc[0]\n",
    "\n",
    "                    split_mean = np.round(np.mean(df_[df_['training']=='MLcl'][columns].iloc[0]), 0)\n",
    "                    local_mean = np.round(np.mean(df_[df_['training']=='MLncl'][columns].iloc[0]), 0)\n",
    "                    none_mean = np.round(np.mean(df_[df_['training']=='MAncl'][columns].iloc[0]), 0)\n",
    "\n",
    "                    split_std = np.round(np.std(df_[df_['training']=='MLcl'][columns].iloc[0]), 0)\n",
    "                    local_std = np.round(np.std(df_[df_['training']=='MLncl'][columns].iloc[0]), 0)\n",
    "                    none_std = np.round(np.std(df_[df_['training']=='MAncl'][columns].iloc[0]), 0)\n",
    "\n",
    "\n",
    "                    # test split - none\n",
    "                    T, p_sn = stats.ttest_rel(split, none, alternative='less')\n",
    "                    if p_sn<0.001:\n",
    "                            sn_sig = \"***\"\n",
    "                    elif p_sn<0.01:\n",
    "                            sn_sig = \"**\"\n",
    "                    elif p_sn<0.05:\n",
    "                            sn_sig = \"*\"\n",
    "                    else:\n",
    "                            sn_sig = \"n.s\"\n",
    "                    # test local - none\n",
    "                    T, p_ln = stats.ttest_rel(local, none, alternative='less')\n",
    "                    if p_ln<0.001:\n",
    "                            ln_sig = \"***\"\n",
    "                    elif p_sn<0.01:\n",
    "                            ln_sig = \"**\"\n",
    "                    elif p_sn<0.05:\n",
    "                            ln_sig = \"*\"\n",
    "                    else:\n",
    "                            ln_sig = \"n.s\"\n",
    "                    # test split - local\n",
    "                    T, p_sl = stats.ttest_rel(split, local, alternative='less')\n",
    "                    if p_sl<0.001:\n",
    "                            sl_sig = \"***\"\n",
    "                    elif p_sl<0.01:\n",
    "                            sl_sig = \"**\"\n",
    "                    elif p_sl<0.05:\n",
    "                            sl_sig = \"*\"\n",
    "                    else:\n",
    "                            sl_sig= \"n.s\"\n",
    "\n",
    "            \n",
    "                    # MAncl vs MLcl\n",
    "\n",
    "                    mae_sn = f\"{split_mean} ± {split_std} ({sn_sig})\"\n",
    "                    mae_ln = f\"{local_mean} ± {local_std} ({ln_sig})\" \n",
    "                    mae_sl = f\"{split_mean} ± {split_std} ({sl_sig})\"  \n",
    "\n",
    "                    df_split_none  = pd.DataFrame(data=[scenario, base_sn, compared_split, leadtime, mae_sn]).transpose()\n",
    "                    df_split_none .columns =  [\"Scenario (Freq, Noise)\", \"base\", \"compared with\", leadtime_column, \"MAE\"]\n",
    "\n",
    "                    df_local_none  = pd.DataFrame(data=[scenario, base_ln, compared_local, leadtime, mae_ln]).transpose()\n",
    "                    df_local_none.columns = [\"Scenario (Freq, Noise)\", \"base\", \"compared with\", leadtime_column, \"MAE\"]\n",
    "\n",
    "                    df_split_local  = pd.DataFrame(data=[scenario, base_sl, compared_split, leadtime, mae_sl]).transpose()\n",
    "                    df_split_local.columns =  [\"Scenario (Freq, Noise)\", \"base\", \"compared with\", leadtime_column, \"MAE\"]\n",
    "                    \n",
    "                    df_final = pd.concat([df_final, df_split_none, df_local_none, df_split_local], axis=0)\n",
    "\n",
    "    scenario_unique = df_final['Scenario (Freq, Noise)'].unique()\n",
    "    scenario_base = df_final['base'].unique()\n",
    "    scenario_cw = df_final['compared with'].unique()\n",
    "\n",
    "    df_system_final = pd.DataFrame()\n",
    "    for scenario in scenario_unique:\n",
    "        for base in scenario_base:\n",
    "            for compared in scenario_cw:\n",
    "                if base == compared:\n",
    "                    pass\n",
    "                else:\n",
    "                    df_ = df_final[(df_final['Scenario (Freq, Noise)']==scenario) & (df_final['base']==base) & (df_final['compared with']==compared)]\n",
    "\n",
    "                    leadtimes_unique = df_['Leadtime'].unique()\n",
    "                    data = [scenario, base, compared]\n",
    "                    df_new = pd.DataFrame(data=data).transpose()\n",
    "                    df_new.columns = ['Scenario (Freq, Noise)', 'base', 'compared with']\n",
    "\n",
    "                    for leadtime in leadtimes_unique:\n",
    "                        col = f\"L = {leadtime}\"\n",
    "                        df_new[col] = df_[df_['Leadtime']==leadtime]['MAE'].iloc[0]\n",
    "\n",
    "                    df_system_final = pd.concat([df_system_final, df_new])\n",
    "\n",
    "    return df_system_final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data frames\n",
    "for metric in metrics:\n",
    "    df_list = []\n",
    "    for file_path in file_paths:\n",
    "        \n",
    "        df_agent = pd.read_excel(file_path, sheet_name= f\"{metric}_raw_run_ind\")\n",
    "        df_agent['training'] = df_agent['training'].replace({'split_multichannel': 'MLcl', 'local_multichannel': 'MLncl', None: 'MAncl'})\n",
    "        df_list.append(df_agent)\n",
    "    df_system = pd.concat(df_list)\n",
    "    df_system_final = create_system_table(df_system=df_system)\n",
    "    \n",
    "    df_system_final.to_excel(f'Results/Evaluation/{metric}_System_PairedT_Test_Results.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create reporting of paired T-test - agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent_table(df_agent):\n",
    "        freq_unique = np.sort(df_agent['freq'].unique())\n",
    "        noise_unique = np.sort(df_agent['noise'].unique())\n",
    "        share_unique = np.sort(df_agent['share'].unique())\n",
    "        df_final = pd.DataFrame()\n",
    "        columns = {}\n",
    "        num_agents = 3\n",
    "        for agent_id in range(num_agents):\n",
    "                column = []\n",
    "                for i in range(RUNS):\n",
    "                    name = \"run_\"+str(i)+\"_agent_\"+str(agent_id)\n",
    "                    column.append(name)\n",
    "                columns[agent_id] = column\n",
    "        for share in share_unique:\n",
    "                df_share = df_agent[df_agent['share']==share]\n",
    "                for freq in freq_unique:\n",
    "                        df_freq = df_share[df_share['freq']==freq]\n",
    "                        for noise in noise_unique:\n",
    "                                df_ = df_freq[df_freq['noise']==noise]\n",
    "                                scenario = return_scenario(freq=freq, noise=noise)\n",
    "\n",
    "                                base_sn = \"MAncl\"\n",
    "                                base_ln = \"MAncl\"\n",
    "                                base_sl = \"MLncl\"\n",
    "                                compared_split = \"MLcl\"\n",
    "                                compared_local = \"MLncl\"\n",
    "\n",
    "                                p_sn_list = []\n",
    "                                p_ln_list = []\n",
    "                                p_sl_list = []\n",
    "                                \n",
    "                                df_sn = pd.DataFrame(data=[scenario, base_sn, compared_split]).transpose()\n",
    "                                df_sn.columns = ['Scenario (Freq, Noise)', 'base', 'compared with']\n",
    "\n",
    "                                df_ln = pd.DataFrame(data=[scenario, base_ln, compared_local]).transpose()\n",
    "                                df_ln.columns = ['Scenario (Freq, Noise)', 'base', 'compared with']\n",
    "\n",
    "                                df_sl = pd.DataFrame(data=[scenario, base_sl, compared_split]).transpose()\n",
    "                                df_sl.columns = ['Scenario (Freq, Noise)', 'base', 'compared with']\n",
    "\n",
    "                                for agent_id in range(num_agents):\n",
    "                                        \n",
    "                                        split = df_[df_['training']=='MLcl'][columns[agent_id]].iloc[0]\n",
    "                                        local = df_[df_['training']=='MLncl'][columns[agent_id]].iloc[0]\n",
    "                                        none = df_[df_['training']=='MAncl'][columns[agent_id]].iloc[0]\n",
    "\n",
    "                                        split_mean = np.round(np.mean(split), 0)\n",
    "                                        local_mean = np.round(np.mean(local), 0)\n",
    "                                        none_mean = np.round(np.mean(none), 0)\n",
    "\n",
    "                                        split_std = np.round(np.std(split), 0)\n",
    "                                        local_std = np.round(np.std(local), 0)\n",
    "                                        none_std = np.round(np.std(none), 0)\n",
    "\n",
    "                                        # test split - none\n",
    "                                        T, p_sn = stats.ttest_rel(split, none, alternative='less')\n",
    "                                        if p_sn<0.001:\n",
    "                                                sn_sig = \"***\"\n",
    "                                        elif p_sn<0.01:\n",
    "                                                sn_sig = \"**\"\n",
    "                                        elif p_sn<0.05:\n",
    "                                                sn_sig = \"*\"\n",
    "                                        else:\n",
    "                                                sn_sig = \"n.s\"\n",
    "                                        # test local - none\n",
    "                                        T, p_ln = stats.ttest_rel(local, none, alternative='less')\n",
    "                                        if p_ln<0.001:\n",
    "                                                ln_sig = \"***\"\n",
    "                                        elif p_sn<0.01:\n",
    "                                                ln_sig = \"**\"\n",
    "                                        elif p_sn<0.05:\n",
    "                                                ln_sig = \"*\"\n",
    "                                        else:\n",
    "                                                ln_sig = \"n.s\"\n",
    "                                        # test split - local\n",
    "                                        T, p_sl = stats.ttest_rel(split, local, alternative='less')\n",
    "                                        if p_sl<0.001:\n",
    "                                                sl_sig = \"***\"\n",
    "                                        elif p_sl<0.01:\n",
    "                                                sl_sig = \"**\"\n",
    "                                        elif p_sl<0.05:\n",
    "                                                sl_sig = \"*\"\n",
    "                                        else:\n",
    "                                                sl_sig= \"n.s\"\n",
    "\n",
    "                                        mae_sn = f\"{split_mean} ± {split_std} ({sn_sig})\"\n",
    "                                        mae_ln = f\"{local_mean} ± {local_std} ({ln_sig})\" \n",
    "                                        mae_sl = f\"{split_mean} ± {split_std} ({sl_sig})\"  \n",
    "                                        \n",
    "                                        col = f\"Agent {agent_id}\"\n",
    "                                        df_sn[col] = mae_sn\n",
    "\n",
    "                                        df_ln[col] = mae_ln\n",
    "\n",
    "                                        df_sl[col] = mae_sl\n",
    "\n",
    "                                        p_sn_list.append(p_sn)\n",
    "                                        p_ln_list.append(p_ln)\n",
    "                                        p_sl_list.append(p_sl)\n",
    "\n",
    "                                df_final = pd.concat([df_final, df_sn, df_ln, df_sl], axis=0)\n",
    "        return df_final\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data frames\n",
    "for metric in metrics:\n",
    "    # Ein neues Workbook erstellen\n",
    "    excel_file_path = f\"Results/Evaluation/{metric}_Agent_Table.xlsx\"\n",
    "    wb = Workbook()\n",
    "\n",
    "    # Die Excel-Datei speichern\n",
    "    wb.save(excel_file_path)\n",
    "    df_list = []\n",
    "    for file_path in file_paths:\n",
    "        \n",
    "        df_agent = pd.read_excel(file_path, sheet_name= \"MAE_raw_agent\")\n",
    "        df_agent['training'] = df_agent['training'].replace({'split_multichannel': 'MLcl', 'local_multichannel': 'MLncl', None: 'MAncl'})\n",
    "        df_list.append(df_agent)\n",
    "    \n",
    "    for i, df_i in enumerate(df_list):\n",
    "        sheet_name = f\"{metric}_agent_table_leadtime_{i}\"\n",
    "        df_final = create_agent_table(df_i)\n",
    "        write(df_final, excel_file_path, sheet_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the figures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create figures for the results at system level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "for metric in metrics:\n",
    "    for p, path in enumerate(paths):\n",
    "        df, df_agent, df_aggregated, config = list_all_subdirectories(path, runs=10, ids=IDS, metric = metric)\n",
    "        df['training'] = df['training'].replace({'split_multichannel': 'ML (colaborative)', \n",
    "                                                'local_multichannel': 'ML (noncolaborative)', \n",
    "                                                'None': 'Moving average (noncolaborative)'})\n",
    "        ### prepare data for barplot\n",
    "        freq_unique = np.sort(df['freq'].unique())\n",
    "        noise_unique = np.sort(df['noise'].unique())\n",
    "        share_unique = np.sort(df['share'].unique())\n",
    "        count = 0 \n",
    "        columns = []\n",
    "        df_barplot = pd.DataFrame()\n",
    "        for i in range(RUNS):\n",
    "            name = \"run_\"+str(i)\n",
    "            columns.append(name)\n",
    "        for i, share in enumerate(share_unique):\n",
    "            df_share = df[df['share']==share]\n",
    "            for j, freq in enumerate(freq_unique):\n",
    "                df_freq = df_share[df_share['freq']==freq]\n",
    "                for k, noise in enumerate(noise_unique):\n",
    "                    df_ = df_freq[df_freq['noise']==noise]\n",
    "                    data = [f\"{freq, noise}\"]*3 # times of different training methods\n",
    "\n",
    "                    data_training = df_['training']\n",
    "                    data_runs = df_[columns]\n",
    "                    data_runs = data_runs.reset_index(drop=True)\n",
    "                    data_final = pd.DataFrame([data, data_training]).transpose()\n",
    "                    data_final.columns = ['Market scenario (freq, noise)', 'Training']\n",
    "                    df_count = pd.concat([data_final,data_runs], axis=1)\n",
    "                    df_barplot = pd.concat([df_barplot, df_count])\n",
    "                    count += 1\n",
    "\n",
    "        ### prepare figure\n",
    "        hue_order = ['Moving average (noncolaborative)', 'ML (noncolaborative)', 'ML (colaborative)']\n",
    "        # Pivoting des DataFrames, um eine lange Form zu erstellen\n",
    "        df_long = df_barplot.melt(id_vars=['Market scenario (freq, noise)', 'Training'], \n",
    "                        value_vars=columns, \n",
    "                        var_name='Messung', value_name='Wert')\n",
    "\n",
    "        # Erstellen eines Gruppen-Balkendiagramms\n",
    "        img_path = f'Results/Evaluation/{metric}_Results_System_leadtime_{(p+1)}.png'\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        if metric == \"MAE\":\n",
    "            plt.ylabel(\"Mean absolute error\", size = 12, fontweight='bold')\n",
    "        if metric == \"MSE\":\n",
    "            plt.ylabel(\"Mean squared error\", size = 12, fontweight='bold')\n",
    "        \n",
    "        ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x):,}'))\n",
    "        \n",
    "        ax = sns.barplot(data=df_long, x='Market scenario (freq, noise)', y='Wert', hue='Training', errorbar='sd', capsize=.1, \n",
    "                    palette=['blue', 'red', 'green'], hue_order = hue_order)\n",
    "        # sns.barplot(data=df_long, x='Market Scenario (Freq, Noise)', y='Wert', hue='Training', errorbar='sd', capsize=.1, \n",
    "        #             hue_order = hue_order)\n",
    "        \n",
    "        plt.legend(loc= 2)\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontweight='bold', size = 12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(img_path)\n",
    "        plt.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create figures for the results at agent level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "for metric in metrics:\n",
    "    for p, path in enumerate(paths):\n",
    "        df, df_agent, df_aggregated, config = list_all_subdirectories(path, runs=10, ids=IDS, metric = metric)\n",
    "        df_agent['training'] = df_agent['training'].replace({'split_multichannel': 'ML (colaborative)', \n",
    "                                                'local_multichannel': 'ML (noncolaborative)', \n",
    "                                                'None': 'Moving average (noncolaborative)'})\n",
    "        ### prepare data for barplot\n",
    "        freq_unique = np.sort(df_agent['freq'].unique())\n",
    "        noise_unique = np.sort(df_agent['noise'].unique())\n",
    "        share_unique = np.sort(df_agent['share'].unique())\n",
    "        training_unique = np.sort(df_agent['training'].unique())\n",
    "        count = 0 \n",
    "        columns = []\n",
    "        df_barplot = pd.DataFrame()\n",
    "        column = []\n",
    "\n",
    "        for run in range(RUNS):\n",
    "                colum_name = \"run_\"+str(run)\n",
    "                column.append(colum_name)\n",
    "\n",
    "                for id in range(IDS):\n",
    "                        colum_name_agent = f\"run_{run}_agent_{id}\"\n",
    "                        columns.append(colum_name_agent)\n",
    "        for i, share in enumerate(share_unique):\n",
    "            df_share = df_agent[df_agent['share']==share]\n",
    "            for j, freq in enumerate(freq_unique):\n",
    "                df_freq = df_share[df_share['freq']==freq]\n",
    "                for k, noise in enumerate(noise_unique):\n",
    "                    df_noise = df_freq[df_freq['noise']==noise]\n",
    "                    for m, training in enumerate(training_unique):\n",
    "                        df_ = df_noise[df_noise['training']==training]\n",
    "                        for id in range(IDS):\n",
    "                            c_list = []\n",
    "                            for run in range(RUNS):\n",
    "                                colum_name_agent = f\"run_{run}_agent_{id}\"\n",
    "                                c_list.append(colum_name_agent)\n",
    "                            \n",
    "                            df_values = df_[c_list]\n",
    "                            df_values = df_values.reset_index(drop=True)\n",
    "                            df_values.columns = column\n",
    "                            data = [f\"{freq, noise}\", training, str(id)] # times of different training methods\n",
    "                            df_data = pd.DataFrame(data).transpose()\n",
    "                            df_data.columns = ['Market scenario (freq, noise)', 'Training', 'Agent']\n",
    "\n",
    "                            df_count = pd.concat([df_data,df_values], axis=1)\n",
    "                            df_barplot = pd.concat([df_barplot, df_count])\n",
    "\n",
    "                    count += 1\n",
    "\n",
    "        ### save figures\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(10, 15)) \n",
    "        hue_order = ['Moving average (noncolaborative)', 'ML (noncolaborative)', 'ML (colaborative)']\n",
    "        # Erstellen eines Gruppen-Balkendiagramms\n",
    "        img_path = f'Results/Evaluation/{metric}_Results_Agent_leadtime_{(p+1)}.png'\n",
    "        \n",
    "        # Pivoting des DataFrames, um eine lange Form zu erstellen\n",
    "        df_long = df_barplot[df_barplot['Agent']=='0'].melt(id_vars=['Market scenario (freq, noise)', 'Training'], \n",
    "                        value_vars=column, \n",
    "                        var_name='Messung', value_name='Wert')\n",
    "\n",
    "        # Erstellen eines Gruppen-Balkendiagramms\n",
    "        if metric == \"MAE\":\n",
    "            axs[0].set_ylabel(\"Mean absolute error\", size = 12, fontweight='bold')\n",
    "        if metric == \"MSE\":\n",
    "            axs[0].set_ylabel(\"Mean squared error\", size = 12, fontweight='bold')\n",
    "\n",
    "        sns.barplot(data=df_long, x='Market scenario (freq, noise)', y='Wert', hue='Training', errorbar='sd', capsize=.1, palette=['blue', 'red', 'green'], hue_order = hue_order, ax = axs[0])\n",
    "        axs[0].set_title('Distribution of MAE for Agent 0 for all market scenarios grouped by forecasting methode')\n",
    "        axs[0].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x):,}'))\n",
    "\n",
    "        df_long = df_barplot[df_barplot['Agent']=='1'].melt(id_vars=['Market scenario (freq, noise)', 'Training'], \n",
    "                        value_vars=column, \n",
    "                        var_name='Messung', value_name='Wert')\n",
    "\n",
    "        # Erstellen eines Gruppen-Balkendiagramms\n",
    "        if metric == \"MAE\":\n",
    "            axs[1].set_ylabel(\"Mean absolute error\", size = 12, fontweight='bold')\n",
    "        if metric == \"MSE\":\n",
    "            axs[1].set_ylabel(\"Mean squared error\", size = 12, fontweight='bold')\n",
    "        sns.barplot(data=df_long, x='Market scenario (freq, noise)', y='Wert', hue='Training', errorbar='sd', capsize=.1, palette=['blue', 'red', 'green'], hue_order = hue_order, ax = axs[1])\n",
    "        axs[1].set_title('Distribution of MAE for Agent 1 for all market scenarios grouped by forecasting methode')\n",
    "        axs[1].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x):,}'))\n",
    "\n",
    "        df_long = df_barplot[df_barplot['Agent']=='2'].melt(id_vars=['Market scenario (freq, noise)', 'Training'], \n",
    "                        value_vars=column, \n",
    "                        var_name='Messung', value_name='Wert')\n",
    "\n",
    "\n",
    "        # Erstellen eines Gruppen-Balkendiagramms\n",
    "        if metric == \"MAE\":\n",
    "            axs[2].set_ylabel(\"Mean absolute error\", size = 12, fontweight='bold')\n",
    "        if metric == \"MSE\":\n",
    "            axs[2].set_ylabel(\"Mean squared error\", size = 12, fontweight='bold')\n",
    "        sns.barplot(data=df_long, x='Market scenario (freq, noise)', y='Wert', hue='Training', errorbar='sd', capsize=.1, palette=['blue', 'red', 'green'], hue_order = hue_order, ax = axs[2])\n",
    "        axs[2].set_title('Distribution of MAE for Agent 2 for all market scenarios grouped by forecasting methode')\n",
    "        axs[2].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x):,}'))\n",
    "\n",
    "        axs[0].legend(loc= 2)\n",
    "        axs[1].legend(loc= 2)\n",
    "        axs[2].legend(loc= 2)\n",
    "        ax.set_xlabel(ax.get_xlabel(), fontweight='bold', size = 12)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(img_path)\n",
    "        plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure for results on real world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: MAE\n",
      "Metric: MSE\n"
     ]
    }
   ],
   "source": [
    "paths = [\"Results/Rawdata/Eval_Realworld_I\", \"Results/Rawdata/Eval_Realworld_II\", \"Results/Rawdata/Eval_Realworld_III\"]\n",
    "\n",
    "metrics = ['MAE', 'MSE']\n",
    "df_dict = {}\n",
    "df_dict_agent = {}\n",
    "for metric in metrics:\n",
    "    print(f\"Metric: {metric}\")\n",
    "    df_final = pd.DataFrame()\n",
    "    df_final_agent = pd.DataFrame()\n",
    "    for i, path in enumerate(paths):\n",
    "        IDS = 3\n",
    "        RUNS = 1\n",
    "        df, df_agent, df_aggregated, config = list_all_subdirectories(path, runs=RUNS, ids=IDS, metric = metric, test_intervall = 19)\n",
    "        df['training'] = df['training'].replace({'split_multichannel': 'ML (colaborative)', \n",
    "                                                'local_multichannel': 'ML (noncolaborative)', \n",
    "                                                'None': 'Moving average (noncolaborative)'})\n",
    "        df_agent['training'] = df_agent['training'].replace({'split_multichannel': 'ML (colaborative)', \n",
    "                                                'local_multichannel': 'ML (noncolaborative)', \n",
    "                                                'None': 'Moving average (noncolaborative)'})\n",
    "        df_final = pd.concat([df_final, df])\n",
    "        df_final_agent = pd.concat([df_final_agent, df_agent])\n",
    "    df_dict[metric] = df_final\n",
    "    df_dict_agent[metric] = df_final_agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create figure of resutls on system level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics:\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    ax = sns.barplot(data=df_dict[metric], x = 'lead_time_l0', y='metric_mean', hue='training', hue_order=hue_order, palette=['blue', 'red', 'green'])\n",
    "    if metric == \"MAE\":\n",
    "        plt.ylabel(\"Mean absolute error\", size = 12, fontweight='bold')\n",
    "    if metric == \"MSE\":\n",
    "        plt.ylabel(\"Mean squared error\", size = 12, fontweight='bold')\n",
    "    \n",
    "\n",
    "    plt.legend(loc= 2)\n",
    "    ax.yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{int(x):,}'))\n",
    "    img_path = f\"Results/Evaluation/Realworld_{metric}_Results_System.png\"\n",
    "    plt.xlabel(\"Lead time\", fontweight='bold', size = 12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create figure of resutls on agent level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_agent_id = 2\n",
    "for metric in metrics:\n",
    "    df = df_dict_agent[metric]\n",
    "\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 10), sharex=True) \n",
    "    hue_order = ['Moving Average (noncolaborative)', 'ML (noncolaborative)', 'ML (colaborative)']\n",
    "    \n",
    "    for i in range(max_agent_id+1):\n",
    "        column = f\"run_0_agent_{i}\"\n",
    "        sns.barplot(data=df, x = 'lead_time_l0', y=column, hue='training', hue_order=hue_order, ax=axs[i], palette=['blue', 'red', 'green'])\n",
    "        axs[i].set_title(f\"Agent {i}\")\n",
    "        axs[i].set_xlabel(\"Lead Time\", fontweight='bold', size = 12)\n",
    "    if metric == \"MAE\":\n",
    "        axs[0].set_ylabel(\"Mean Absolute Error\", size = 12, fontweight='bold')\n",
    "        axs[1].set_ylabel(\"Mean Absolute Error\", size = 12, fontweight='bold')\n",
    "        axs[2].set_ylabel(\"Mean Absolute Error\", size = 12, fontweight='bold')\n",
    "    if metric == \"MSE\":\n",
    "        axs[0].set_ylabel(\"Mean Squared Error\", size = 12, fontweight='bold')\n",
    "        axs[1].set_ylabel(\"Mean Squared Error\", size = 12, fontweight='bold')\n",
    "        axs[2].set_ylabel(\"Mean Squared Error\", size = 12, fontweight='bold')\n",
    "        \n",
    "    plt.legend(loc= 2)\n",
    "    img_path = f\"Results/Evaluation/Realworld{metric}_Results_Agent.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
